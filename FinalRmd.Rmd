---
title: "BST260 Final Project - COVID-19 and Politics"
author: "Natalie Averkamp, Anja Shahu, and Kirstie Turnbull"
date: "12/9/2020"
output: html_document
---

**Overview and Motivation**

This project was motivated by the most pressing and impactful events this year: the COVID-19 pandemic and the U.S. presidential election. It has been widely recognized that the Trump administration has failed to act appropriately to contain the COVID-19 virus^1,2,3^, and we were interested in quantifying and visualizing associations between political actions and COVID-19 in the United States. Specifically, we were interested in developing a multiple linear regression model for prediction of COVID-19 cases, an interactive visualization of state-wide policies related to COVID-19 and COVID-19 cases, and an interactive visualization of changes in COVID-19 prevalence after Trump campaign rallies were held in certain counties.

1. Yamey Gavin, Gonsalves Gregg. Donald Trump: a political determinant of covid-19 BMJ 2020; 369 :m1643 (https://www.bmj.com/content/369/bmj.m1643)
2. Paul E. Rutledge, Associate Professor and Chair, Department of Civic Engagement and Public Service, University of West Georgia, 1601 Maple St. Carrollton, GA 30118, USA. Email: prutledg@westga.edu (https://journals.sagepub.com/doi/full/10.1177/0275074020941683)
3. Fabian Hattke, Helge Martin. (2020) Collective action during the Covid-19 pandemic: The case of Germanyâ€™s fragmented authority. Administrative Theory & Praxis 0:0, pages 1-19. (https://www.tandfonline.com/doi/citedby/10.1080/10841806.2020.1750212?scroll=top&needAccess=true)

**Related Work**

We were inspired both by current events and by the Gapminder example that we saw in class, in which a Shiny application captured the change in infant mortality for many countries over time, while also capturing national GDP for each country. In our visualizations, we hoped to capture multiple variables in a similar way.

**Initial Questions**

One of our initial questions was: Is mobility (as measured by Google as percent change from baseline, with the baseline being December and January 2019) a useful predictor of COVID-19 cases? Our thought process with this question within the scope of our project focusing on COVID-19 and politics was that unfortunately, due to President Trump frequently mocking those who chose to stay at home during the pandemic, mobility/lack thereof was, in a way, a political action. This question evolved over the course of the project because we discovered that Carnegie Mellon University had developed a measurement of the percentage of people wearing masks in public. We were also interested in including mask-use in our model to see if it was a useful predictor of COVID-19 cases. Ultimately, our best model for prediction did not include the mask-use variable, so this question evolved again as we tried to understand why the mask-use data were not helpful when it came to COVID-19 case prediction in our sample. Additional questions that we had included: how did state mandates and stay-at home orders influence the number of COVID-19 cases, and did the number of COVID-19 cases in a given county or neighboring county increase after that county had held a Trump campaign rally?

**Data**

The data we used for this project came from many sources. State population, governor's political party, state racial demographics, the number of cities in the top 50 largest cities in the state, state population density data and Trump 2020 election rallies were web-scraped from Wikipedia. Google mobility data came from Google's [website](https://www.google.com/covid19/mobility/). COVID-19 data was pulled from the New York Times [GitHub](https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv). Mask-use data came from Carnegie Mellon University's [website](https://delphi.cmu.edu/covidcast/?sensor=fb-survey-smoothed_wearing_mask&level=county&date=20201205&signalType=value&encoding=color&mode=overview&region=42003). Data on county-level populations came from the
[US Census Bureau](https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/). Adjacent county data also came from the 
[US Census Bureau](https://www.census.gov/geographies/reference-files/2010/geo/county-adjacency.html). Data connecting counties to their corresponding city came from 
[simplemaps](https://simplemaps.com/data/us-cities). A dataset connecting states to their two letter abbreviation was also used and can be found
[here](https://worldpopulationreview.com/states/state-abbreviations). 
Policy data was pulled from the
[HHS-curated policy data set](https://healthdata.gov/dataset/covid-19-state-and-county-policy-orders). 

```{r, warning = FALSE, message = FALSE}
# loading relavent libraries
library(tidyverse)
library(lubridate)
library(rvest)
library(stringr)
library(zoo)
library(directlabels)
library(splines)
library(splines2)
library(plotly)
library(knitr)
library(ggthemes)
```

```{r}
##### load state population data from wikipedia and make it easier to use
# this is the url we want
url <- "https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States_by_population" 
# read the url
h <- read_html(url) 
# pull out our desired table
tab <- h %>% html_nodes("table") %>% .[1] %>% html_table(fill = TRUE) %>% .[[1]] 
# choose only the state and population columns
pop_states <- tab %>% select("State","Census population")
# get rid of unnecessary rows
pop_states <- pop_states[2:57,]
# make "state" a factor
pop_states$State <- as.factor(pop_states$State)
# remove commas from population and make it numeric
pop_states$`Census population` <- as.numeric(str_replace_all(pop_states$`Census population`, ",", ""))
# rename the columns to make them more useable
colnames(pop_states) <- c("state","population")

##### load Google mobility data from the downloadable CSV and make it easier to use (from https://www.google.com/covid19/mobility/)
# load the CSV
mobility <- read_csv("data/2020_US_Region_Mobility_report.csv")
# we only want the state-level data because some of our other variables do not have county-level data, remove county-level data
mobility <- mobility %>% filter(is.na(sub_region_2)==TRUE)
# remove whole U.S.-level data
mobility <- mobility %>% filter(is.na(sub_region_1)==FALSE)
# rename the state column so that I can join the data frames easily
colnames(mobility)[3] <- "state"
# make "state" a factor
mobility$state <- as.factor(mobility$state)

##### load NYT COVID data and make it more useable
# this is the CSV url we want
url <- "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv"
# load the CSV url
covid <- read_csv(url)
# make "state" a factor
covid$state <- as.factor(covid$state)

##### load state governor's political party from wikipedia
# this is the url we want
url <- "https://en.wikipedia.org/wiki/List_of_current_United_States_governors"
# load the url
h <- read_html(url)
# pull the table that we want
tab <- h %>% html_nodes("table") %>% .[1] %>% html_table(fill = TRUE) %>% .[[1]]
# remove unnecessary rows
gov_political <- tab[2:51,1:5]
# rename columns to make them more useable
colnames(gov_political)[1] <- "state"
colnames(gov_political)[5] <- "gov_party"
# we only want state and governor's party variables
gov_political <- gov_political %>% select("state","gov_party")
# fix some strange entries for two states
gov_political[23,2] <- "Democratic"
gov_political[48,2] <- "Republican"
# make state and political party factors 
gov_political$state <- as.factor(gov_political$state)
gov_political$gov_party <- as.factor(gov_political$gov_party)

##### load state racial demographic (% of pop. that is a certain race) data from wikipedia (note: we are especially interested in % Black/African American and % American Indian/Alaskan Native because we know that these populations have been disproportionately impacted by COVID due to systemic racism)
# this is the url we want
url <- "https://en.wikipedia.org/wiki/Demographics_of_the_United_States"
# read in the url
h <- read_html(url)
# pull out the table that we want
tab <- h %>% html_nodes("table") %>% .[28] %>% html_table(fill = TRUE) %>% .[[1]]
# rename data frame
racial_demog <- tab
# make state a factor
racial_demog$`State or territory` <- as.factor(racial_demog$`State or territory`)
# replace %s in % Black or African American column
racial_demog$`Black orAfrican American` <- str_replace_all(racial_demog$`Black orAfrican American`, "%", "")
# make % Black or African American numeric
racial_demog$`Black orAfrican American` <- as.numeric(racial_demog$`Black orAfrican American`)
# replace %s in % American Indian/Alaskan Native column
racial_demog$`American Indianand Alaska Native` <- str_replace_all(racial_demog$`American Indianand Alaska Native`, "%", "")
# make % American Indian/Alaskan Native numeric
racial_demog$`American Indianand Alaska Native` <- as.numeric(racial_demog$`American Indianand Alaska Native`)

##### load mask-use CSV from Carnegie Mellon University (https://delphi.cmu.edu/covidcast/?sensor=fb-survey-smoothed_wearing_mask&level=county&date=20201205&signalType=value&encoding=color&mode=overview&region=42003)
# load CSV 
mask <- read.csv("data/covidcast-fb-survey-smoothed_wearing_mask-2020-04-06-to-2020-12-01.csv")
# recode state names
list <- c("ak"="Alaska","al"="Alabama","az"="Arizona","ar"="Arkansas","ca"="California","co"="Colorado","ct"="Connecticut","de"="Delaware","fl"="Florida","ga"="Georgia","hi"="Hawaii","ia"="Iowa","id"="Idaho","il"="Illinois","in"="Indiana","ks"="Kansas","ky"="Kentucky","la"="Louisiana","ma"="Massachusetts","md"="Maryland","me"="Maine","mi"="Michigan","mn"="Minnesota","mo"="Missouri","ms"="Mississippi","mt"="Montana","nc"="North Carolina","ne"="Nebraska","nh"="New Hampshire","nj"="New Jersey","nm"="New Mexico","nv"="Nevada","ny"="New York","oh"="Ohio","ok"="Oklahoma","or"="Oregon","pa"="Pennsylvania","sc"="South Carolina","tn"="Tennessee","tx"="Texas","ut"="Utah","va"="Virginia","wa"="Washington","wi"="Wisconsin","wv"="West Virginia","ri"="Rhode Island","sd"="South Dakota","nd"="North Dakota","vt"="Vermont","wy"="Wyoming")
mask <- mask %>% mutate(state=recode(geo_value,!!!list))
# make state a factor
mask$state <- as.factor(mask$state)
# make date a date
mask$time_value <- ymd(mask$time_value)
# rename columns to make them more useable
colnames(mask)[4] <- "date"
colnames(racial_demog)[1] <- "state"

##### load large city (number of cities in top 50 largest cities per state) data from wikipedia
# here is the url
url <- "https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population"
# read the url
h <- read_html(url)
# pull the table we want
tab <- h %>% html_nodes("table") %>% .[5] %>% html_table(fill = TRUE) %>% .[[1]]
# rename dataframe
large_cities <- tab
# pull the column and rows of interest
large_cities <- large_cities[1:50,3]
# make state a factor
large_cities <- as.factor(large_cities)
# count up number of large cities per state
large_cities <- as.data.frame(table(large_cities))
# rename columns to make them more useable
colnames(large_cities) <- c("state","num_large_cities")
# filter to exclude DC
large_cities <- large_cities %>% filter(state != "District of Columbia")

##### load population density data from wikipedia
# here is the url
url <- "https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States_by_population_density"
# read the url
h <- read_html(url)
# pull the desired table
tab <- h %>% html_nodes("table") %>% .[2] %>% html_table(fill = TRUE) %>% .[[1]]
# rename data frame
pop_density <- tab
# we only need these rows and columns
pop_density <- pop_density[2:57,c(1,4)]
# rename columns to make them easier to use
colnames(pop_density)[1] <- "state"
colnames(pop_density)[2] <- "density"
# make density numeric
pop_density$density <- as.numeric(pop_density$density)

##### make final data frame
# join all the data frames
final_df <- left_join(gov_political,pop_states,by="state")
final_df <- left_join(final_df,racial_demog,by="state")
final_df <- left_join(mobility,final_df,by="state")
final_df <- left_join(final_df,covid,by=c("state","date"))
final_df <- left_join(mask,final_df,by=c("state","date"))
final_df <- left_join(final_df,large_cities,by="state")
final_df <- left_join(final_df,pop_density,by="state")
# add a time column that is days since 9/7/20 (the day before our mask data starts)
final_df$time <- as.numeric(final_df$date - ymd("2020-09-07"))
# make introduced NAs in large cities column 0s
final_df$num_large_cities <- str_replace_na(final_df$num_large_cities,replacement = 0)
# that made the column characters, have to make it numeric again
final_df$num_large_cities <- as.numeric(final_df$num_large_cities)
# filter to days before 11/25 - the last day of covid data that we pulled
final_df <- final_df %>% filter(date < ymd("2020-11-25"))
# make sure state is a factor
final_df$state <- as.factor(final_df$state)
# remove dc as a state
final_df <- final_df %>% filter(state != "dc")
# remove extra columns that we aren't interested in 
final_df <- final_df %>% select(c(date,value,state,retail_and_recreation_percent_change_from_baseline,grocery_and_pharmacy_percent_change_from_baseline,parks_percent_change_from_baseline,transit_stations_percent_change_from_baseline,workplaces_percent_change_from_baseline,residential_percent_change_from_baseline,gov_party,population,`Black orAfrican American`,`American Indianand Alaska Native`,cases,num_large_cities,density,time))
```


**Exploratory Analysis**

```{r}
# run a summary of the final data frame for the model
summary(final_df)
```
```{r}
# some initial plots/scatter.smooths for visualization
par(mfrow=c(2,2))
scatter.smooth(final_df$value,log(final_df$cases),col="pink",xlab="Mask Wearing (%)",ylab="log(cases)")
scatter.smooth(final_df$retail_and_recreation_percent_change_from_baseline,log(final_df$cases),col="pink",xlab="Retail/Recreation Mobility Change (%)",ylab="log(cases)")
scatter.smooth(final_df$grocery_and_pharmacy_percent_change_from_baseline,log(final_df$cases),col="pink",xlab="Grocery/Pharmacy Mobility Change (%)",ylab="log(cases)")
scatter.smooth(final_df$parks_percent_change_from_baseline,log(final_df$cases),col="pink",xlab="Parks Mobility Change (%)",ylab="log(cases)")
```

```{r}
# some initial plots/scatter.smooths for visualization
par(mfrow=c(2,2))
scatter.smooth(final_df$transit_stations_percent_change_from_baseline,log(final_df$cases),col="pink",xlab="Transit Mobility Change (%)",ylab="log(cases)")
scatter.smooth(final_df$workplaces_percent_change_from_baseline,log(final_df$cases),col="pink",xlab="Workplace Mobility Change (%)",ylab="log(cases)")
scatter.smooth(final_df$residential_percent_change_from_baseline,log(final_df$cases),col="pink",xlab="Residential Mobility Change (%)",ylab="log(cases)")
plot(final_df$gov_party,log(final_df$cases),xlab="Governor's Political Party",ylab="log(cases)")
```

```{r}
# some initial plots/scatter.smooths for visualization
par(mfrow=c(3,2))
scatter.smooth(log(final_df$population),log(final_df$cases),col="pink",xlab="log(population",ylab="log(cases)")
scatter.smooth(log(final_df$`Black orAfrican American`),log(final_df$cases),col="pink",xlab="log(% Black or African American)",ylab="log(cases)")
scatter.smooth(log(final_df$`American Indianand Alaska Native`),log(final_df$cases),col="pink",xlab="log(% American Indian or Alaska Native",ylab="log(cases)")
plot(final_df$num_large_cities,log(final_df$cases),col="pink",xlab="Number of Large Cities",ylab="log(cases)")
scatter.smooth(log(final_df$density),log(final_df$cases),col="pink",xlab="log(population density)",ylab="log(cases)")
```

From these exploratory statistics and data, we can see the following that might help inform how to adjust the prediction model:

* Looks like as the percentage of people wearing masks in public increases, log(cases) increases and then decreases. We may want to add a polynomial or spline term.
* Looks like as retail mobility increases, log(cases) increases and then decreases. We may want to add a polynomial or spline term.
* Looks like as grocery store mobility increases, log(cases) increases and then decreases. We may want to try a polynomial or spline term.
* Looks like as parks mobility increases, log(cases) decreases.
* Looks like as transit mobility increases, log(cases) increases and then decreases. We may want to try a polynomial or spline term.
* As workplace mobility increases, log(cases) does not appear to change much.
* Looks like as residential mobility increases, log(cases) increases, but barely. 
* It looks like log(cases) is distributed pretty equally among states with Democrat and Republican governors
* As log(population) increases, log(cases) increases.
* As log(percentage of the population that is Black or African American) increases, log(cases) increases.
* As log(percentage of the population that is American Indian or Alaskan Native) increases, log(cases) decreases - this probably has more to do with population density in states that have larger populations of indigenous peoples.
* Looks like as the number of cities in the top 50 larges cities by population increases, log(cases) increases. 
* Looks like as log(population density) increases, log(cases) increases. 


**Final Analysis**

Due to our smaller dataset (constricted by the number of days that mask-use, a primary variable of interest for us, is available for), we want to train the model on 90% of the data and test on 10%. Due to the time element of the data, this translates to training on the data from 9/8/2020 to 11/15/2020 and testing on the data from 11/15/2020 to 11/24/2020.

```{r}
# split data into training and testing sets
train_set <- final_df %>% filter(time < 70)
test_set <- final_df %>% filter(time >= 70)
```

The initial model included all 6 mobility variables, mask-use, governor's party, population density, the number of large cities, the proportion of state population that is Black or African American, the proportion of the population that is American Indian or Alaska Native, and a spline of days since 9/7/2020, with an offset of the log of the population, in order to predict the log of total cases. 

```{r}
# run initial model and check the summary
mod1 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             grocery_and_pharmacy_percent_change_from_baseline + 
             parks_percent_change_from_baseline + 
             transit_stations_percent_change_from_baseline +
             workplaces_percent_change_from_baseline +
             residential_percent_change_from_baseline +
             value +
             gov_party + 
             density +
             num_large_cities +
             `Black orAfrican American` +
             `American Indianand Alaska Native` +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
summary(mod1)
```

From this inital model, it appears that nearly all of the variables are associated with log(cases) in the training set (though, notably, grocery/pharmacy mobility change and governor's political party are not). In order to assess how well the model predicts in the test set, we can calculate the RMSE and assess an observed vs. predicted plot. 

```{r}
# observed vs. predicted plot
mod1_preds <- predict.lm(mod1,test_set)
plot(exp(mod1_preds),test_set$cases,xlab="Predicted Number of Cases",ylab="Observed Number of Cases",col="skyblue",main="Observed vs. Predicted for Model 1")
abline(0,1,col="navy")
```

```{r}
# calculate RMSE
sqrt(mean(((exp(mod1_preds) - test_set$cases)^2), na.rm=TRUE))
```


From the observed vs. predicted plot above, we can see that this model generally under-predicts COVID-19 cases, meaning that the predicted number of cases is less than the observed number of cases. This discrepancy is also captured in the RMSE, which is quite high at 135,824.7 (to add context to the size of the RMSE, we are predicting the total number of cases). With the benefit of hindsight, it does logically make sense that we would under-predict cases from 11/15 to 11/24, because there was such a massive international COVID-19 spike occurring around that time. However, that RMSE can likely be decreased more by updating the model based on what was observed in the data exploration plots. Several more models were tested (see the un-included/un-evaluated chunk in FinalRmd.Rmd file for details), and the best model for prediction that was found is as follows:

```{r,eval=FALSE,include=FALSE}
# remove governor party
mod2 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             grocery_and_pharmacy_percent_change_from_baseline + 
             parks_percent_change_from_baseline + 
             transit_stations_percent_change_from_baseline +
             workplaces_percent_change_from_baseline +
             residential_percent_change_from_baseline +
             value +
             density +
             num_large_cities +
             `Black orAfrican American` +
             `American Indianand Alaska Native` +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
summary(mod2)
mod2_preds <- predict.lm(mod2,test_set)
plot(exp(mod2_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod2_preds) - test_set$cases)^2), na.rm=TRUE))

# remove grocery mobility
mod3 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             parks_percent_change_from_baseline + 
             transit_stations_percent_change_from_baseline +
             workplaces_percent_change_from_baseline +
             residential_percent_change_from_baseline +
             value +
             density +
             num_large_cities +
             `Black orAfrican American` +
             `American Indianand Alaska Native` +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
summary(mod3)
mod3_preds <- predict.lm(mod3,test_set)
plot(exp(mod3_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod3_preds) - test_set$cases)^2), na.rm=TRUE))
# worse for prediction

# add quadratic term for retail
mod4 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             I(retail_and_recreation_percent_change_from_baseline^2) +
             grocery_and_pharmacy_percent_change_from_baseline + 
             parks_percent_change_from_baseline + 
             transit_stations_percent_change_from_baseline +
             workplaces_percent_change_from_baseline +
             residential_percent_change_from_baseline +
             value +
             density +
             num_large_cities +
             `Black orAfrican American` +
             `American Indianand Alaska Native` +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
mod4_preds <- predict.lm(mod4,test_set)
plot(exp(mod4_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod4_preds) - test_set$cases)^2), na.rm=TRUE))
# worse for prediction

# add a quadratic term for grocery mobility
mod5 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             grocery_and_pharmacy_percent_change_from_baseline + 
             I(grocery_and_pharmacy_percent_change_from_baseline^2) +
             parks_percent_change_from_baseline + 
             transit_stations_percent_change_from_baseline +
             workplaces_percent_change_from_baseline +
             residential_percent_change_from_baseline +
             value +
             density +
             num_large_cities +
             `Black orAfrican American` +
             `American Indianand Alaska Native` +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
mod5_preds <- predict.lm(mod5,test_set)
plot(exp(mod5_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod5_preds) - test_set$cases)^2), na.rm=TRUE))
# better for prediction

# add transit mobility quadratic term
mod6 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             grocery_and_pharmacy_percent_change_from_baseline + 
             I(grocery_and_pharmacy_percent_change_from_baseline^2) +
             parks_percent_change_from_baseline + 
             transit_stations_percent_change_from_baseline +
             I(transit_stations_percent_change_from_baseline^2) +
             workplaces_percent_change_from_baseline +
             residential_percent_change_from_baseline +
             value +
             density +
             num_large_cities +
             `Black orAfrican American` +
             `American Indianand Alaska Native` +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
mod6_preds <- predict.lm(mod6,test_set)
plot(exp(mod6_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod6_preds) - test_set$cases)^2), na.rm=TRUE))
# worse for prediction

# remove workplace mobility 
mod7 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             grocery_and_pharmacy_percent_change_from_baseline + 
             I(grocery_and_pharmacy_percent_change_from_baseline^2) +
             parks_percent_change_from_baseline + 
             transit_stations_percent_change_from_baseline +
             residential_percent_change_from_baseline +
             value +
             density +
             num_large_cities +
             `Black orAfrican American` +
             `American Indianand Alaska Native` +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
mod7_preds <- predict.lm(mod7,test_set)
plot(exp(mod7_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod7_preds) - test_set$cases)^2), na.rm=TRUE))
# better for prediction

# remove residential mobility
mod8 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             grocery_and_pharmacy_percent_change_from_baseline + 
             I(grocery_and_pharmacy_percent_change_from_baseline^2) +
             parks_percent_change_from_baseline + 
             transit_stations_percent_change_from_baseline +
             value +
             density +
             num_large_cities +
             `Black orAfrican American` +
             `American Indianand Alaska Native` +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
mod8_preds <- predict.lm(mod8,test_set)
plot(exp(mod8_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod8_preds) - test_set$cases)^2), na.rm=TRUE))
# better for prediction

# add value (mask) quadratic term
mod9 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             grocery_and_pharmacy_percent_change_from_baseline + 
             I(grocery_and_pharmacy_percent_change_from_baseline^2) +
             parks_percent_change_from_baseline + 
             transit_stations_percent_change_from_baseline +
             value +
             I(value^2) +
             density +
             num_large_cities +
             `Black orAfrican American` +
             `American Indianand Alaska Native` +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
mod9_preds <- predict.lm(mod9,test_set)
plot(exp(mod9_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod9_preds) - test_set$cases)^2), na.rm=TRUE))
# worse for prediction

# remove transit mobility
mod10 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             grocery_and_pharmacy_percent_change_from_baseline + 
             I(grocery_and_pharmacy_percent_change_from_baseline^2) +
             parks_percent_change_from_baseline + 
             value +
             density +
             num_large_cities +
             `Black orAfrican American` +
             `American Indianand Alaska Native` +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
mod10_preds <- predict.lm(mod10,test_set)
plot(exp(mod10_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod10_preds) - test_set$cases)^2), na.rm=TRUE))

# take log of % Black or AA
mod11 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             grocery_and_pharmacy_percent_change_from_baseline + 
             I(grocery_and_pharmacy_percent_change_from_baseline^2) +
             parks_percent_change_from_baseline + 
             value +
             density +
             num_large_cities +
             log(`Black orAfrican American`) +
             `American Indianand Alaska Native` +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
mod11_preds <- predict.lm(mod11,test_set)
plot(exp(mod11_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod11_preds) - test_set$cases)^2), na.rm=TRUE))

# take log of % Am Indian/AK native
mod12 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             grocery_and_pharmacy_percent_change_from_baseline + 
             I(grocery_and_pharmacy_percent_change_from_baseline^2) +
             parks_percent_change_from_baseline + 
             value +
             density +
             num_large_cities +
             log(`Black orAfrican American`) +
             log(`American Indianand Alaska Native`) +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
mod12_preds <- predict.lm(mod12,test_set)
plot(exp(mod12_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod12_preds) - test_set$cases)^2), na.rm=TRUE))
# worse for prediction

# remove mask wearing value
mod13 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             grocery_and_pharmacy_percent_change_from_baseline + 
             I(grocery_and_pharmacy_percent_change_from_baseline^2) +
             parks_percent_change_from_baseline +
             num_large_cities +
             density + 
             log(`Black orAfrican American`) +
             `American Indianand Alaska Native` +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
mod13_preds <- predict.lm(mod13,test_set)
plot(exp(mod13_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod13_preds) - test_set$cases)^2), na.rm=TRUE))
# better for prediction

# remove density 
mod14 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             grocery_and_pharmacy_percent_change_from_baseline + 
             I(grocery_and_pharmacy_percent_change_from_baseline^2) +
             parks_percent_change_from_baseline +
             num_large_cities +
             log(`Black orAfrican American`) +
             `American Indianand Alaska Native` +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
mod14_preds <- predict.lm(mod14,test_set)
plot(exp(mod14_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod14_preds) - test_set$cases)^2), na.rm=TRUE))
# better for prediction

# remove % Am Indian/AK Native
mod15 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             grocery_and_pharmacy_percent_change_from_baseline + 
             I(grocery_and_pharmacy_percent_change_from_baseline^2) +
             parks_percent_change_from_baseline +
             num_large_cities +
             log(`Black orAfrican American`) +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
mod15_preds <- predict.lm(mod15,test_set)
plot(exp(mod15_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod15_preds) - test_set$cases)^2), na.rm=TRUE))
# better for prediction

# remove quadratic term for grocery mobility
mod16 <- lm(log(cases) ~ retail_and_recreation_percent_change_from_baseline + 
             grocery_and_pharmacy_percent_change_from_baseline + 
             parks_percent_change_from_baseline +
             num_large_cities +
             log(`Black orAfrican American`) +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
mod16_preds <- predict.lm(mod16,test_set)
plot(exp(mod16_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod16_preds) - test_set$cases)^2), na.rm=TRUE))

# remove retail mobility 
mod17 <- lm(log(cases) ~  
             grocery_and_pharmacy_percent_change_from_baseline + 
             parks_percent_change_from_baseline +
             num_large_cities +
             log(`Black orAfrican American`) +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
mod17_preds <- predict.lm(mod17,test_set)
plot(exp(mod17_preds),test_set$cases)
abline(0,1)
sqrt(mean(((exp(mod17_preds) - test_set$cases)^2), na.rm=TRUE))
```

```{r}
mod18 <- lm(log(cases) ~  
             parks_percent_change_from_baseline +
             num_large_cities +
             log(`Black orAfrican American`) +
             bSpline(time,df=5),
             offset = log(population), 
           data=train_set)
summary(mod18)
mod18_preds <- predict.lm(mod18,test_set)
plot(exp(mod18_preds),test_set$cases,xlab="Predicted Number of Cases",ylab="Observed Number of Cases",col="skyblue",main="Observed vs. Predicted for Model 18")
abline(0,1,col="navy")
sqrt(mean(((exp(mod18_preds) - test_set$cases)^2), na.rm=TRUE))
```

As can be seen in the above observed vs. predicted plot, the model still under-predicts cases, however, this final model appears to bring the values much closer to the 1:1 observed vs. predicted line. Additionally, the RMSE has decreased- it is still quite large but it is now 115,593.3 instead of 135,824.7. One interesting aspect of this final prediction model is that the mask-use data, which we anticipated would be useful for prediction, is not included. Additionally, many of the mobility change variables were also not useful for prediction. In order to try to understand why mask-use data was not useful, we created a map of the United States shaded by the percentage of people reporting wearing a mask in public on 11/19/2020, and then compared that visualization to a plot of the total number of cases per state on that same date. 

```{r}
library(maps)
states <- map_data("state")
final_df$state <- tolower(final_df$state)
colnames(states)[5] <- "state"
map_df <- left_join(final_df,states,by="state")
map_df %>% filter(time==73) %>% ggplot(aes(x = long, y = lat, group = group)) +
   geom_polygon(aes(fill=value), color = "white") + 
   theme(panel.grid.major = element_blank(), 
         panel.background = element_blank(),
         axis.title = element_blank(), 
         axis.text = element_blank(),
         axis.ticks = element_blank()) +
   ggtitle("Percent of People Reporting They Wear a Mask in Public on 11/19/20") + 
   scale_fill_viridis_c(name="")
```

```{r}
final_df <- final_df %>% mutate(total_case_rate = (cases/population)*100000)
final_df %>% filter(time==73) %>% ggplot(aes(x=state,y=total_case_rate)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90)) + xlab("State") + ylab("Total COVID-19 Cases per 100,000") + ggtitle("Total COVID-19 Cases by State on 11/19/2020")
```

We had hypothesized that in a very basic sense, the higher the mask-use, the lower the number of COVID-19 cases. So, based on this snapshot, that would mean that because the Northeast states have the highest mask-use, they would have lower COVID-19 prevalence, which does not really appear to be the case based on the above plot. It would also mean that states like South Dakota, Wyoming, and Idaho, which had much lower mask-use, would have higher COVID-19 prevalence, which does appear to be true. There are several possible reasons why mask-use was not a useful predictor of COVID-19 cases, and two specific reasons that we think could be contributing factors here are that the mask-use data were collected via Facebook polls, which could be inconsistent, and there were only about two months-worth of observations, so perhaps more data would be needed to thoroughly capture the relationship between the two.

In the accompanying Shiny applications, we investigated state-wide policies related to COVID-19. 

```{r}
# wrangling code for policy part of Shiny app

# read in state-level cases and deaths data from nytimes
covid <- read_csv("data/covid.csv")

# read in policy data
policies <- read_csv("data/policies.csv")

# download population data 
url <- "https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States_by_population"
h <- read_html(url)

# extract data from wiki population table
population_names <- c("rank_curr", "rank_2010", "state", "pop_2019", "pop_2010", "perc_change", "abs_change", "house_seats", "house_perc", "est_pop_per_elec_vote", "cen_pop_per_seat_2019", "cen_pop_per_seat_2010", "perc_pop_2019", "perc_pop_2010", "perc_pop_change", "perc_of_elec")
population <- h %>% 
    html_nodes("table") %>% 
    .[1] %>% 
    html_table(fill = TRUE) %>% 
    .[[1]] %>% 
    setNames(population_names)

# removed extra row at the top and total rows at the bottom of data frame and row for American Samoa in population data
population <- population %>% 
    slice(2:56) %>% 
    select(state, pop_2019) %>% 
    rename(population = pop_2019)

# remove out [10], [12] and [14] subscripts from last 3 rows and remove commas from population numbers
# also change population to numeric class
population <- population %>% 
    mutate(population = str_replace_all(population, "\\[\\d{2}\\]", ""),
           population = str_replace_all(population, ",", ""),
           population = as.numeric(population))

# change to U.S. Virgin Islands to Virgin Islands to match covid data frame 
population <- population %>% 
    mutate(state = recode(state, 
                          `U.S. Virgin Islands` = "Virgin Islands"))

# create tidy data for covid data
covid_tidy <- covid %>% 
    gather(measure, cum_count, cases:deaths)

# create 7 day rolling average of cases and deaths
covid_tidy <- covid_tidy %>% 
    group_by(state, measure) %>%
    arrange(state, measure, date) %>%
    mutate(new_count = cum_count - lag(cum_count),
           new_count_7dayavg = rollmean(new_count, k = 7, fill = NA)) %>%
    ungroup()

# join population data to covid data frame
covid_tidy <- covid_tidy %>% left_join(population, by = "state")

# create per 10 million population variables for 7 day average variables
covid_tidy <- covid_tidy %>% 
    mutate(new_count_7dayavg_per_1mil = new_count_7dayavg / (population / 1000000),
           cum_count_per_100thous = cum_count / (population / 100000))

# remove county-level policies data to keep only state-level policies 
policies_subset <- policies %>% 
    filter(policy_level == "state") %>%
    rename(state = state_id)

# removed row that was incorrectly labeled as state-level policy
policies_subset <- policies_subset %>% 
    filter(is.na(county)) %>% 
    select(-c(source, fips_code, county, policy_level))

# change from abbreviated to complete state names in policy data frame
covid_df_state_names <- sort(unique(covid$state))
policies_df_state_names <- c("AL", "AK", "AZ", "AR", "CA", 
                             "CO", "CT", "DE", "DC", "FL", 
                             "GA", "GU", "HI", "ID", "IL", 
                             "IN", "IA", "KS", "KY", "LA", 
                             "ME", "MD", "MA", "MI", "MN", 
                             "MS", "MO", "MT", "NE", "NV", 
                             "NH", "NJ", "NM", "NY", "NC", 
                             "ND", "MP", "OH", "OK", "OR", 
                             "PA", "PR", "RI", "SC", "SD", 
                             "TN", "TX", "UT", "VT", "VI", 
                             "VA", "WA", "WV", "WI", "WY")
key <- setNames(covid_df_state_names, policies_df_state_names)

policies_subset <- policies_subset %>% 
    mutate(state = recode(state, !!!key))

# join combined covid and population data with policies data
tidy_df <- covid_tidy %>% left_join(policies_subset, by = c("state", "date"))

# create wide_df for summary tables in app 
wide_df <- tidy_df %>%
    pivot_wider(names_from = measure,
                values_from = c(cum_count, new_count,
                                new_count_7dayavg,
                                new_count_7dayavg_per_1mil, 
                                cum_count_per_100thous))

# create table with ranks for policies
policies_rank <- policies_subset %>% 
    filter(start_stop == "start") %>%
    group_by(state) %>%
    summarise(n_policy = n()) %>%
    arrange(desc(n_policy)) %>% 
    mutate(rank_policy = as.numeric(rownames(.)), 
           policy_comb = paste0(rank_policy, " (", n_policy, ")")) %>%
    ungroup()

# create table for ranks of cum_cases, cum_deaths, cum_cases_per_100thous, and cum_deaths_per_100thous
cum_ranks <- covid_tidy %>%
    pivot_wider(names_from = measure,
                values_from = c(cum_count, new_count,
                                new_count_7dayavg,
                                new_count_7dayavg_per_1mil, 
                                cum_count_per_100thous)) %>%
    filter(date == max(date)) %>%
    select(state, cum_count_cases, cum_count_deaths,
           cum_count_per_100thous_cases, 
           cum_count_per_100thous_deaths) %>%
    arrange(desc(cum_count_cases)) %>% 
    mutate(rank_cum_cases = as.numeric(rownames(.)),
           cum_cases_comb = paste0(rank_cum_cases, " (",
                                   cum_count_cases, ")")) %>% 
    arrange(desc(cum_count_deaths)) %>%
    mutate(rank_cum_deaths = as.numeric(rownames(.)),
           cum_deaths_comb = paste0(rank_cum_deaths, " (",
                                    cum_count_deaths, ")")) %>%
    arrange(desc(cum_count_per_100thous_cases)) %>% 
    mutate(rank_cum_cases_per_100thous = as.numeric(rownames(.)),
           cum_cases_per_100thous_comb = paste0(rank_cum_cases_per_100thous, " (",
                                                round(cum_count_per_100thous_cases,
                                                      digits = 2), ")")) %>%
    arrange(desc(cum_count_per_100thous_deaths)) %>% 
    mutate(rank_cum_deaths_per_100thous = as.numeric(rownames(.)),
           cum_deaths_per_100thous_comb = paste0(rank_cum_deaths_per_100thous, " (",
                                                 round(cum_count_per_100thous_deaths,
                                                       digits = 2), ")"))
# merge rank tables 
rank_table <- policies_rank %>% left_join(cum_ranks, by = "state")

# import US map data
us_map <- map_data("state")

# update states to match spelling from tidy_df
us_map <- us_map %>% 
    rename(state = region) %>% 
    mutate(state = str_to_title(state)) %>%
    mutate(state = str_replace(state, "District Of Columbia", "District of Columbia"))
```

In our app, we provide plots of each state that mark the start/stop of the state's policies. From these plots, we found that there was no consistently clear immediate impact of starting/stopping a policy on the rate of cases and deaths across the U.S. states. However, we found a similar pattern where the number of state-wide policies being passed has gone noticeably down as the pandemic has progressed. 

```{r}
max_lim_policies_plot <- tidy_df %>%
        pull(new_count_7dayavg) %>% max(na.rm = TRUE)

policies_plot_filtered <- tidy_df %>%
  filter(state %in% c("Florida", "California"), !is.na(policy_type)) %>%
  mutate(plot_position = max_lim_policies_plot)

tidy_df %>%
  filter(state %in% c("Florida", "California")) %>%
  ggplot(aes(x = date)) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  geom_vline(data = policies_plot_filtered,
             aes(xintercept = date), color = "darkgrey", lty = 2) + # add vertical line for days where policy was stopped/started in that selected state
  geom_point(data = policies_plot_filtered,
             aes(y = plot_position, color = start_stop),
             size = 2) +
  geom_line(aes(y = new_count_7dayavg, color = measure),
            show.legend = FALSE) +
  geom_dl(aes(y = new_count_7dayavg, label = measure, color = measure),
          method = list("last.points", cex = 0.90)) + # label lines to designate cases line and deaths line
  facet_wrap(~ state, nrow = 2) +
  scale_y_log10(limits = c(1, max_lim_policies_plot)) + # set limits so y axis doesn't change as selected state is changed
  ggtitle(paste("New cases and deaths in Florida & California over time")) +
  xlab("Date") +
  ylab("Count (7-day average) (log10 scale)") +
  theme_bw() +
  scale_colour_brewer(type = "qual", palette = "Paired",
                      name = "Policy started or stopped?",
                      breaks = c("start", "stop"),
                      labels = c("Started", "Stopped")) + # use brewer color palette for cases/deaths lines and policy lines
  theme(legend.position = "bottom") 
```

To allow for inter-state comparison, we tried to quantify policy activity by calculating the number of state-wide policies that were passed in each state. We created a table that provided rankings for states in terms of number of policies passed, number of cumulative cases and number of cumulative deaths. The two top 10 tables below are ordered in ascending order using data from December 10, 2020, but the first one uses the raw number of cumulative cases and the second uses the number of cumulative cases/100k individuals. We can see that only 2/10 of the states with the worst raw numbers of cumulative cases and only 3/10 from the states with the top 10 worst cumulative cases/100k individuals are inside the list of top 25 states with most policies passed. Therefore, there could potentially be an inverse association between number of policies passed and cumulative COVID-19 cases. However, the absolute differences in the number of policies are not very large between states, so a more formal test would likely be needed to make any conclusions.

```{r}
table1 <- rank_table %>%
  arrange(rank_cum_cases) %>%
  select(state, policy_comb, cum_cases_comb, cum_deaths_comb) %>%
  rename(State = state,
         `Policies ranking (# passed)` = policy_comb,
         `Cumulative cases ranking (# cases)` = cum_cases_comb,
         `Cumulative deaths ranking (# deaths)` = cum_deaths_comb) %>%
  head(n = 10) 

kable(table1)

table2 <- rank_table %>%
  arrange(rank_cum_cases_per_100thous) %>%
  select(state, policy_comb, cum_cases_per_100thous_comb, cum_deaths_per_100thous_comb) %>%
  rename(State = state,
         `Policies ranking (# passed)` = policy_comb,
         `Cumulative cases/100k ranking (# cases)` = cum_cases_per_100thous_comb,
         `Cumulative deaths/100k ranking (# deaths)` = cum_deaths_per_100thous_comb) %>%
  head(n = 10) 

kable(table2)
```

We think that there are a number of reasons for the lack of clear patterns or conclusions in our policy analysis. First, our data set looked broadly at all COVID-19-related policies, meaning that some policies were more substantive and relevant than others and likely would have a greater impact on case numbers. In the future it could be helpful to filter the data and look at a specific type of policy (e.g. shelter-in-place/lockdown policies or business/restaurant closures) and assess the impact of those policies. Second, the relationship between policies and cases and deaths is very state-dependent for numerous reasons. For example, some states might defer decision-making to counties, while other states might pass state-wide policies but might not allocate funding to enforce those policies. We think that future research might find more success by focusing on a specific state rather than looking broadly across all U.S. states; with this approach, information on county-wide policies could also be incorporated, which could prove to be illuminating. Third, exploring policies is complicated and using a quick metric like number of policies passed, while easy to understand, means we lose some nuance. Fourth, we think that there are limitations in the data set we used; it seems that the data is not complete and future work should focus on curating a more complete data set.


In the accompanying Shiny app, we also looked at the post-Trump rally county COVID-19 prevalence.

```{r}
###Trump rally app wrangling code 

#scrape wikipedia data of 2020 election Trump rallies
url <- "https://en.wikipedia.org/wiki/List_of_post-election_Donald_Trump_rallies"
#extract table and remove day of week
h <- read_html(url)
tab <- h %>% html_nodes("table") %>% .[8]
tab <- tab %>% html_table() %>% .[[1]]
test <- tab$`Date of rally`
test <- test %>% str_replace("Monday, |Tuesday, |Wednesday, |Thursday, |Friday, |Saturday, |Sunday, ", "")

#next problem is the [###] at the end of the dates.
#all rows have at least one, so remove the first one
test <- test %>% substr(1, nchar(test) - 5)

#now, if they have more [], we remove them
for (i in c(1:4)){
  pattern = nchar(test) > 20
  test[pattern] <- test[pattern] %>% substr(1, nchar(test[pattern]) -5)
}


#convert to dates
test <- mdy(test)
test <- test[1:67]
tab$`Date of rally` <- test

#get city and covid data
cities <- read.csv("data/uscities.csv")
covid <- read.csv("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv")

#combine rally data and city data 
new_test <- left_join(tab, cities, by=c("State" ="state_id", "City" = "city"))
#three  didn't match due to being townships not real cities
new_test[60, 9] <- "Macomb"
new_test[52, 9] <- "Oakland"
new_test[18, 9] <- "Beaver"

#renaming columns and changing dates to date types
new_test <- new_test %>% select(c("Date of rally", "City", "State", "county_name", "population", "state_name")) %>% rename(date = "Date of rally")
covid$date <- ymd(covid$date)
new_test$date <- ymd(new_test$date)
```

In addition to looking at Covid-19 cases in the counties that held Trump rallies, we thought it would be interesting to include the counties surrounding the location of the rally. Using the US Census Bureau county adjcaency file, we were able to easily match counties with their neighboring counties. 

```{r}
#adjacent county data 
adjacent <- read.table("https://www2.census.gov/geo/docs/reference/county_adjacency.txt", sep="\t", fill=FALSE, strip.white=TRUE)[,c(1,3)]
adjacent <- adjacent %>% rename(county=V1,adj_counties=V3)
adjacent$adj_counties[adjacent$adj_counties == "Do\xb1a Ana County, NM"] <- "Dona Ana County, NM"
adjacent$county[adjacent$county == "Do\xb1a Ana County, NM"] <- "Dona Ana County, NM"

#removing Puerto Rico since there were not rallies there
#and it was causing problems
adjacent <- adjacent[1:21721,]

#adjusting the data so that it can be used easier in the app
#filling in county name down so that the data can be filtered by 
#county name and all adjacent counties will appear
for (i in (1:nrow(adjacent))){
    if (i != (nrow(adjacent))){
        if (nchar(adjacent$county[i+1])==0){
            adjacent$county[i+1]<-adjacent$county[i]
        }
    }
}

#County names are in format "County name County, IA"
#Need to extract county name and state
adjacent$county_name <- substr(adjacent$county, 1, nchar(adjacent$county)-11)
adjacent$county_state <- substr(adjacent$county, nchar(adjacent$county)-2, nchar(adjacent$county))
adjacent$adj_county_name <- substr(adjacent$adj_counties, 1, nchar(adjacent$adj_counties)-11)
adjacent$adj_county_state <- substr(adjacent$adj_counties, nchar(adjacent$adj_counties)-1, nchar(adjacent$adj_counties))

#data set connecting state abbreviations to state names
#combining this data set with adjacency data
state_abs <- read_csv("data/state_abbrevs.csv")
adjacent <- left_join(adjacent, state_abs, by=c("adj_county_state" = "Code"))

#census data with county populations from 2019
county_pop <- read_csv("data/county-pop.csv")
county_pop <- county_pop %>% select(c("STNAME", "CTYNAME", "POPESTIMATE2019")) %>% 
    rename("state"="STNAME", "county"="CTYNAME", "population"="POPESTIMATE2019")
county_pop$county[1835]<-"Dona Ana County"
county_pop$county[county_pop$county=="Cars"]<- "Carson City"
county_pop$county <- substr(county_pop$county, 1, nchar(county_pop$county)-7)

#Combining the NYT Covid data to the county populations and 
#calculating cases per thousand people 
covid <- left_join(covid, county_pop, by=c("county", "state"))
covid <- covid %>% mutate(cases_per_thous = cases*1000 / population)

#Adding column for shiny app to display cities as city, state
new_test <- new_test %>% mutate(city_state = paste(City, State, sep=", "))


#Sample plot 
rally_date <- new_test$date[new_test$county_name=="Lehigh"]
adj_data <- adjacent %>% filter(county_name == "Lehigh", State=="Pennsylvania")
p <- covid %>% filter(county %in% adj_data$adj_county_name, state %in% adj_data$State) %>% ggplot(aes(x=date, y=cases_per_thous, group=county, color=county)) + geom_line() + scale_x_date(date_breaks="1 month", date_labels="%b %d") + geom_vline(xintercept=rally_date,linetype=4) + theme_few() + ggtitle("Covid-19 Cases in Lehigh County and Surrounding Counties") + ylab("Cases per Thousand")
p
```

In many of the counties that held Trump rallies, there were spikes in cases following the rally. Of course, seeing a spike in cases after a single rally does not constitute causation. However, the trend of case spikes in nearly all of the cities with Trump rallies combined with the information that the rallies drew crowds, many of whom were not wearing masks, does lead one to believe the rallies may have caused a significant number of cases. Additionally, the increase in cases in many cities that held Trump rallies was marked. 

The accompanying Shiny app contains graphs of cases for each city that held a Trump 2020 election rally. There are options to view cases and cases per thousand people as well as cases in the counties surrounding the city of the rally. 


